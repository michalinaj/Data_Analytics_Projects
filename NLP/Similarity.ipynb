{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2875 entries, 0 to 2874\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   AUTHOR    2873 non-null   object \n",
      " 1   JOURNAL   2777 non-null   object \n",
      " 2   TITLE     2875 non-null   object \n",
      " 3   YEAR      2866 non-null   float64\n",
      " 4   ABSTRACT  2875 non-null   object \n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 112.4+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#dataset includes a list of scientific research articles\n",
    "df = pd.read_csv('completed_clean_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2875"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text clean-up\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def normalize_document(doc):\n",
    "  doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "  doc = doc.lower()\n",
    "  doc = doc.strip()\n",
    "  doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
    "  tokens = nltk.word_tokenize(doc)\n",
    "  filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "  doc = ' '.join(filtered_tokens)\n",
    "  return doc\n",
    "  \n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(list(df['ABSTRACT']))\n",
    "len(norm_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the cosine similarity between the abstracts of the attached documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2875, 30660)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tfidf_matrix = tf.fit_transform(norm_corpus)\n",
    "tfidf_matrix.shape\n",
    "\n",
    "#cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2865</th>\n",
       "      <th>2866</th>\n",
       "      <th>2867</th>\n",
       "      <th>2868</th>\n",
       "      <th>2869</th>\n",
       "      <th>2870</th>\n",
       "      <th>2871</th>\n",
       "      <th>2872</th>\n",
       "      <th>2873</th>\n",
       "      <th>2874</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022779</td>\n",
       "      <td>0.032152</td>\n",
       "      <td>0.028440</td>\n",
       "      <td>0.062236</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>0.009565</td>\n",
       "      <td>0.011017</td>\n",
       "      <td>0.006810</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>0.041680</td>\n",
       "      <td>0.008736</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.016563</td>\n",
       "      <td>0.039453</td>\n",
       "      <td>0.047954</td>\n",
       "      <td>0.023440</td>\n",
       "      <td>0.048615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022779</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024258</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>0.025037</td>\n",
       "      <td>0.018276</td>\n",
       "      <td>0.018347</td>\n",
       "      <td>0.047968</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>0.012694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005068</td>\n",
       "      <td>0.007235</td>\n",
       "      <td>0.014425</td>\n",
       "      <td>0.034319</td>\n",
       "      <td>0.029571</td>\n",
       "      <td>0.023079</td>\n",
       "      <td>0.029015</td>\n",
       "      <td>0.051089</td>\n",
       "      <td>0.032311</td>\n",
       "      <td>0.011365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.032152</td>\n",
       "      <td>0.024258</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043408</td>\n",
       "      <td>0.040312</td>\n",
       "      <td>0.007826</td>\n",
       "      <td>0.026156</td>\n",
       "      <td>0.017533</td>\n",
       "      <td>0.014365</td>\n",
       "      <td>0.024885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011575</td>\n",
       "      <td>0.013655</td>\n",
       "      <td>0.014750</td>\n",
       "      <td>0.016498</td>\n",
       "      <td>0.028598</td>\n",
       "      <td>0.018393</td>\n",
       "      <td>0.022295</td>\n",
       "      <td>0.013188</td>\n",
       "      <td>0.035566</td>\n",
       "      <td>0.028252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028440</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>0.043408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077761</td>\n",
       "      <td>0.008686</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>0.008669</td>\n",
       "      <td>0.031760</td>\n",
       "      <td>0.017833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.012177</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.019169</td>\n",
       "      <td>0.020668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>0.004610</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.062236</td>\n",
       "      <td>0.025037</td>\n",
       "      <td>0.040312</td>\n",
       "      <td>0.077761</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.015062</td>\n",
       "      <td>0.015367</td>\n",
       "      <td>0.023163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019208</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>0.012678</td>\n",
       "      <td>0.023929</td>\n",
       "      <td>0.007849</td>\n",
       "      <td>0.016027</td>\n",
       "      <td>0.038139</td>\n",
       "      <td>0.028702</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>0.020856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2875 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  1.000000  0.022779  0.032152  0.028440  0.062236  0.005699  0.009565   \n",
       "1  0.022779  1.000000  0.024258  0.013722  0.025037  0.018276  0.018347   \n",
       "2  0.032152  0.024258  1.000000  0.043408  0.040312  0.007826  0.026156   \n",
       "3  0.028440  0.013722  0.043408  1.000000  0.077761  0.008686  0.009958   \n",
       "4  0.062236  0.025037  0.040312  0.077761  1.000000  0.003917  0.008909   \n",
       "\n",
       "       7         8         9     ...      2865      2866      2867      2868  \\\n",
       "0  0.011017  0.006810  0.000595  ...  0.001298  0.004186  0.041680  0.008736   \n",
       "1  0.047968  0.002435  0.012694  ...  0.005068  0.007235  0.014425  0.034319   \n",
       "2  0.017533  0.014365  0.024885  ...  0.011575  0.013655  0.014750  0.016498   \n",
       "3  0.008669  0.031760  0.017833  ...  0.002148  0.012177  0.001676  0.019169   \n",
       "4  0.015062  0.015367  0.023163  ...  0.019208  0.004549  0.012678  0.023929   \n",
       "\n",
       "       2869      2870      2871      2872      2873      2874  \n",
       "0  0.002084  0.016563  0.039453  0.047954  0.023440  0.048615  \n",
       "1  0.029571  0.023079  0.029015  0.051089  0.032311  0.011365  \n",
       "2  0.028598  0.018393  0.022295  0.013188  0.035566  0.028252  \n",
       "3  0.020668  0.000000  0.006758  0.004610  0.001366  0.000000  \n",
       "4  0.007849  0.016027  0.038139  0.028702  0.013295  0.020856  \n",
       "\n",
       "[5 rows x 2875 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc_sim = cosine_similarity(tfidf_matrix)\n",
    "doc_sim_df = pd.DataFrame(doc_sim)\n",
    "doc_sim_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our moving recommender - pick a single article (under TITLE) and recommend five other related articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['some problems of tei markup and early printed books',\n",
       "       'the design of the tei encoding scheme',\n",
       "       'modifying the tei dtd the case of korean dictionaries',\n",
       "       'the encoding of spoken texts', 'the tei history goals and future'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Article recommender based on movie recommender\n",
    "def article_recommender(title, articles, doc_sims):\n",
    "  article_idx = np.where(articles == title)[0][0]\n",
    "  article_similarities = doc_sims.iloc[article_idx].values\n",
    "  similar_article_idxs = np.argsort(-article_similarities)[1:6]\n",
    "  similar_articles = articles[similar_article_idxs]\n",
    "  return similar_articles\n",
    "  \n",
    "article_recommender('encoding textual criticism',\n",
    "  df['TITLE'].values,\n",
    "  doc_sim_df\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion Questions\n",
    "Describe a set of texts and research question that interests you that could be explored using this method. Basically, what is a potential application of this method to another area of research?\n",
    "\n",
    "ANSWER: Among multiple fields that use this method, my idea could be to see if there is a significant difference between short text written by English native speakers and people for whom it is a “second language”. The method would be to sample texts from both groups and then generate similar text samples. Based on the algorithm generated similar text samples, we would than be able to identify if the supplier of these was a native speaker or not. Potentially this could be then further developed with a unsupervised learning algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
